# Dockerfile for Local LLM Service
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies and uv
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && curl -LsSf https://astral.sh/uv/install.sh | sh

# Add uv to PATH (updated path for newer versions)
ENV PATH="/root/.local/bin:$PATH"

# Copy project files
COPY pyproject.toml uv.lock* ./

# Install Python dependencies using uv (CPU version by default)
RUN uv sync --extra cpu --no-dev

# Download Gemma-2-2B model weights directly into the container
ARG HUGGINGFACE_TOKEN
ENV HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}

# Copy download script
COPY download_model.py ./

# Download model using huggingface_hub (handles authentication properly)
RUN uv run python download_model.py

# Copy application code
COPY . .

# Set environment for local model weights
ENV LOCAL_MODEL_PATH=./weights
ENV DEVICE=auto
ENV USE_QUANTIZATION=false

# Expose port
EXPOSE 8001

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8001/health || exit 1

# Run the application using uv
CMD ["uv", "run", "main.py"]
